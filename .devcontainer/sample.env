# ============================================================================
# Claude Code multi-provider sample.env
#
# Goal: avoid hybrid configs.
# - GCP uses native Vertex AI (no LiteLLM).
# - Ollama uses cc-proxy (Claude talks to localhost:3456).
# - Copilot uses LiteLLM proxy (Claude talks to localhost:4000).
#
# This file is a TEMPLATE. The active file is `.devcontainer/.env`.
# ============================================================================

DEVCONTAINER=true
CLAUDE_CODE_VERSION=latest

# ----------------------------------------------------------------------------
# Provider selector (used by scripts/manage.py)
# Options: gcp | ollama-proxy | copilot | kimi-k25
# ----------------------------------------------------------------------------
PROFILE=gcp

# ============================================================================
# GCP (native Vertex AI) - used when PROFILE=gcp
# ============================================================================
# Required:
GCP_PROJECT_ID=your-gcp-project-id
CLOUD_ML_REGION=global

# Optional but recommended (native Vertex model IDs, NOT aliases like "sonnet"):
# Examples:
# - claude-opus-4-5@20251101
# - claude-sonnet-4-5@20250929
# - claude-haiku-4-5@20250925
ANTHROPIC_DEFAULT_OPUS_MODEL=claude-opus-4-5@20251101
ANTHROPIC_DEFAULT_SONNET_MODEL=claude-sonnet-4-5@20250929
ANTHROPIC_DEFAULT_HAIKU_MODEL=claude-haiku-4-5@20250925
CLAUDE_CODE_SUBAGENT_MODEL=claude-sonnet-4-5@20250929

# Notes:
# - No LITELLM_* or ANTHROPIC_BASE_URL is needed/used for GCP native mode.
# - ADC auth is handled by `gcloud auth application-default login`.

# ============================================================================
# LiteLLM proxy (used when PROFILE=copilot)
# ============================================================================
# Required for proxy mode:
LITELLM_PORT=4000
# Generate: openssl rand -hex 32
LITELLM_MASTER_KEY=your-litellm-master-key

# Which LiteLLM config to use (scripts/manage.py will set this on setup):
# LITELLM_CONFIG=config.copilot.yaml
LITELLM_CONFIG=config.copilot.yaml

# ============================================================================
# Moonshot Kimi K2.5 (Anthropic-compatible, PROFILE=kimi-k25)
# ============================================================================
# Required:
MOONSHOT_AI_API_KEY=your-moonshot-api-key

# ============================================================================
# Ollama via cc-proxy (PROFILE=ollama-proxy)
# ============================================================================
# - Ollama must run on your HOST machine (outside the devcontainer)
# - Start: ollama serve
# - Pull:  ollama pull llama3.1:70b llama3.1:8b qwen2.5:3b
# - The container reaches it via host.docker.internal:11434
#
# Phase 0/1: cc-proxy is a stub (health + auth), no Ollama calls yet.
CC_PROXY_PORT=3456
# Replace this placeholder (or let scripts/manage.py generate a key during setup).
CC_PROXY_AUTH_KEY=your-cc-proxy-key

# ============================================================================
# GitHub Copilot notes (PROFILE=copilot)
# ============================================================================
# - Ensure youâ€™re logged into Copilot in VS Code/Cursor
# - LiteLLM uses the Copilot session to access Claude models
